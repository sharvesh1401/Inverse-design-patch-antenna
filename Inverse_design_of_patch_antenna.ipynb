{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtQLpD2VZsjK8XgFc2rNvX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Inverse Design of Patch Antennas - Polished Workload-Ready Version\n",
        "# ======================================================\n",
        "\n",
        "# 1. Environment Setup\n",
        "# ---------------------\n",
        "!pip install -q tensorflow numpy pandas matplotlib scikit-learn keras-tuner gradio\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras_tuner as kt\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set random seeds for full reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 2. Dataset Generation (Physics-Constrained)\n",
        "# --------------------------------------------\n",
        "c = 3e8  # Speed of light (m/s)\n",
        "N = 30000  # Number of antenna samples\n",
        "\n",
        "# Design parameters\n",
        "f = np.random.uniform(1e9, 12e9, size=N)\n",
        "er = np.random.uniform(2.2, 12.0, size=N)\n",
        "h = np.random.uniform(0.5e-3, 3e-3, size=N)\n",
        "loss_tangent = np.random.uniform(0.0001, 0.02, size=N)\n",
        "\n",
        "# Analytical formulas for patch antenna\n",
        "W = np.clip(c / (2*f) * np.sqrt(2/(er+1)), 1e-3, 300e-3)\n",
        "eps_eff = (er+1)/2 + (er-1)/(2*np.sqrt(1+12*h/W))\n",
        "delta_L = 0.412*h*((eps_eff+0.3)*(W/h+0.264))/((eps_eff-0.258)*(W/h+0.8))\n",
        "L = np.clip(c / (2*f*np.sqrt(eps_eff)) - 2*delta_L, 1e-3, 300e-3)\n",
        "\n",
        "# Add realistic fabrication noise\n",
        "W_noisy = W * np.random.normal(1.0, 0.015, size=N)\n",
        "L_noisy = L * np.random.normal(1.0, 0.015, size=N)\n",
        "\n",
        "# Build dataset with physical enforcement: W >= 1.05*L\n",
        "data = pd.DataFrame({\n",
        "    'f_GHz': f/1e9,\n",
        "    'W_mm': np.maximum(W_noisy*1e3, L_noisy*1e3 * 1.05),\n",
        "    'L_mm': L_noisy*1e3,\n",
        "    'er': er,\n",
        "    'h_mm': h*1e3,\n",
        "    'loss_tangent': loss_tangent\n",
        "})\n",
        "\n",
        "# 3. Data Preparation\n",
        "# --------------------\n",
        "X = data[['f_GHz', 'er']].values\n",
        "y = data[['L_mm', 'W_mm']].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_scaler = StandardScaler().fit(X_train)\n",
        "y_scaler = StandardScaler().fit(y_train)\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "# 4. Model Definition (Physics-Aware Loss)\n",
        "# ----------------------------------------\n",
        "def custom_loss(y_true, y_pred):\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    constraint_penalty = tf.reduce_mean(tf.square(tf.maximum(1.05*y_pred[:,0] - y_pred[:,1], 0)))\n",
        "    return mse + 0.001 * constraint_penalty\n",
        "\n",
        "def constraint_satisfaction_metric(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.cast(y_pred[:,1] >= 1.05 * y_pred[:,0], tf.float32)) * 100\n",
        "\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(\n",
        "            units=hp.Int('units1', 128, 512, step=64), activation='relu', input_shape=(2,)),\n",
        "        tf.keras.layers.Dropout(hp.Float('dropout', 0.1, 0.5)),\n",
        "        tf.keras.layers.Dense(\n",
        "            units=hp.Int('units2', 64, 256, step=64), activation='relu'),\n",
        "        tf.keras.layers.Dense(2)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])),\n",
        "        loss=custom_loss,\n",
        "        metrics=['mae', constraint_satisfaction_metric]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 5. Hyperparameter Tuning (Keras Tuner Polished)\n",
        "# ------------------------------------------------\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective(\"val_constraint_satisfaction_metric\", direction=\"max\"),\n",
        "    max_trials=15,\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True,\n",
        "    directory='tuner_results',\n",
        "    project_name='patch_antenna_inverse_design'\n",
        ")\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_constraint_satisfaction_metric', patience=5, mode='max', restore_best_weights=True)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "\n",
        "# Start tuning\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "tuner.search(X_train_scaled, y_train,\n",
        "             epochs=60,\n",
        "             batch_size=128,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[early_stop, reduce_lr],\n",
        "             verbose=2)\n",
        "\n",
        "# 6. Best Model Training (Fine-tuned)\n",
        "# -----------------------------------\n",
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "# Full training on best model\n",
        "print(\"Retraining best model...\")\n",
        "history = best_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=150,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 7. Evaluation\n",
        "# --------------\n",
        "y_pred_scaled = best_model.predict(X_test_scaled)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Enforce constraint at prediction\n",
        "y_pred[:,1] = np.maximum(y_pred[:,1], y_pred[:,0]*1.05)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "compliance = constraint_satisfaction_metric(tf.convert_to_tensor(y_test, dtype=tf.float32), tf.convert_to_tensor(y_pred, dtype=tf.float32)).numpy()\n",
        "\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "print(f\"MAE: {mae:.3f} mm\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "print(f\"Physics Compliance: {compliance:.2f}%\")\n",
        "\n",
        "# 8. Visualization\n",
        "# -----------------\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['constraint_satisfaction_metric'], label='Train Compliance')\n",
        "plt.plot(history.history['val_constraint_satisfaction_metric'], label='Val Compliance')\n",
        "plt.title('Physics Constraint Satisfaction (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_curves.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# 9. Saving Model and Hyperparameters\n",
        "# ------------------------------------\n",
        "if not os.path.exists('outputs'):\n",
        "    os.makedirs('outputs')\n",
        "\n",
        "best_model.save(\"outputs/final_patch_antenna_model.keras\")\n",
        "\n",
        "with open(\"outputs/best_hyperparameters.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        'units1': best_hp.get('units1'),\n",
        "        'units2': best_hp.get('units2'),\n",
        "        'dropout': best_hp.get('dropout'),\n",
        "        'learning_rate': best_hp.get('learning_rate')\n",
        "    }, f, indent=4)\n",
        "\n",
        "# 10. Gradio Interface (After training ends)\n",
        "# -------------------------------------------\n",
        "def plot_antenna(L, W):\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    ax.add_patch(plt.Rectangle((0,0), L, W, fill=False, edgecolor='blue', linewidth=2))\n",
        "    ax.set_xlim(0, max(L,W)+10)\n",
        "    ax.set_ylim(0, max(L,W)+10)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(f\"Patch Antenna\\nL: {L:.1f} mm | W: {W:.1f} mm\")\n",
        "    ax.grid(True)\n",
        "    return fig\n",
        "\n",
        "def predict_patch(f_GHz, er):\n",
        "    X_input = X_scaler.transform([[f_GHz, er]])\n",
        "    pred = best_model.predict(X_input, verbose=0)\n",
        "    L, W = y_scaler.inverse_transform(pred)[0]\n",
        "    W = max(W, 1.05*L)\n",
        "    text_output = f\"Predicted L = {L:.2f} mm, W = {W:.2f} mm\"\n",
        "    return text_output, plot_antenna(L, W)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_patch,\n",
        "    inputs=[\n",
        "        gr.Slider(1.0, 12.0, value=2.4, step=0.1, label=\"Frequency (GHz)\"),\n",
        "        gr.Slider(2.2, 12.0, value=4.4, step=0.1, label=\"Dielectric Constant (Îµáµ£)\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Dimensions\"),\n",
        "        gr.Plot(label=\"Patch Antenna Geometry\")\n",
        "    ],\n",
        "    title=\"Patch Antenna Designer\",\n",
        "    description=\"Deep Learning-based Inverse Design | Physics-Constrained | Heavy Dataset\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1007
        },
        "id": "nuudhU3UCz9L",
        "outputId": "cb185c32-c00b-4d67-cedd-7e5f1c19eb98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 11s]\n",
            "val_constraint_satisfaction_metric: 100.0\n",
            "\n",
            "Best val_constraint_satisfaction_metric So Far: 100.0\n",
            "Total elapsed time: 00h 02m 58s\n",
            "Retraining best model...\n",
            "Epoch 1/150\n",
            "150/150 - 3s - 19ms/step - constraint_satisfaction_metric: 99.9740 - loss: 189.1965 - mae: 8.4241 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 35.4863 - val_mae: 3.9178 - learning_rate: 5.0000e-04\n",
            "Epoch 2/150\n",
            "150/150 - 1s - 6ms/step - constraint_satisfaction_metric: 100.0000 - loss: 30.8969 - mae: 3.4402 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 26.8794 - val_mae: 3.2600 - learning_rate: 5.0000e-04\n",
            "Epoch 3/150\n",
            "150/150 - 1s - 7ms/step - constraint_satisfaction_metric: 100.0000 - loss: 24.6879 - mae: 3.0029 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 21.9152 - val_mae: 2.8568 - learning_rate: 5.0000e-04\n",
            "Epoch 4/150\n",
            "150/150 - 1s - 7ms/step - constraint_satisfaction_metric: 100.0000 - loss: 19.8984 - mae: 2.5540 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 16.8597 - val_mae: 2.2518 - learning_rate: 5.0000e-04\n",
            "Epoch 5/150\n",
            "150/150 - 1s - 8ms/step - constraint_satisfaction_metric: 100.0000 - loss: 15.1704 - mae: 2.0063 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 12.4540 - val_mae: 1.6821 - learning_rate: 5.0000e-04\n",
            "Epoch 6/150\n",
            "150/150 - 2s - 10ms/step - constraint_satisfaction_metric: 100.0000 - loss: 11.1622 - mae: 1.6139 - val_constraint_satisfaction_metric: 100.0000 - val_loss: 9.1884 - val_mae: 1.3363 - learning_rate: 5.0000e-04\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\n",
            "=== Evaluation Results ===\n",
            "MAE: 212.790 mm\n",
            "RÂ² Score: -359.6479\n",
            "Physics Compliance: 100.00%\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8ba5999aaee6a7fa74.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8ba5999aaee6a7fa74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}